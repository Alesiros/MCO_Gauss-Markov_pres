---
title: "MCO y errores del modelo de acuerdo a los supuestos del Teorema Gauss-Markov." 
author: "Ch√°vez Huapeo Jacqueline, Flores Ochoa Sofia Libertad, Mendoza Esteban Lizzet, L√≥pez Carmona Audrey Carolina, Rosas Moreno Alesi"
format: revealjs
---

#### M√≠nimos Cuadrados Ordinarios(MCO)

Es una regresi√≥n lineal com√∫n donde se obtiene estimaciones de par√°metros, que describe la relaci√≥n entre una o m√°s variables cuantitativas independiente y una variable dependiente.

**¬øC√≥mo se estima?** Suponemos $$Y_i=\hat{\beta_1}+\hat{\beta_2}Xi+\hat{u_i}$$ y la parte de los residuos los definimos como: $$\hat{u_i}=Y_i-\hat{Y_i}=Y_i-\hat{\beta_1}-\hat{\beta_2}Xi$$ $$\sum\hat{u_i}=\sum(Y_i-\hat{Y_i})^2$$

# 

Con eso definido, buscamos reducir la diferencia $$\frac{\partial (Y_i-\hat{\beta_1}-\hat{\beta_2}Xi)^2}{\partial \hat{\beta_1}}=2\sum(Y_i\hat\beta_1-\hat\beta_2X_i)=0$$ hay que observar que suponemos que n observaciones por lo tanto: $$\sum Y_i-n\hat\beta_1-\beta_2\sum X_i=0$$ y despejamos:

$$\hat\beta_1=\bar Y+ \hat\beta_2\bar X$$

# 

Para el caso de $\hat\beta_2$

$\frac{\partial (Y_i-\hat{\beta_1}-\hat{\beta_2}Xi)^2}{\partial \hat{\beta_2}}=2\sum(Y_i\hat\beta_1-\hat\beta_2X_i)X_i=-2\sum\hat u_iX_i=0$ desarollamos y reemplazamos el valor de $\hat\beta_1$

$$\sum(X_i Y_i)=\hat\beta_1\sum(X_i)+\hat\beta_2\sum(X_i^2)$$

$$\sum(X_i Y_i)-\bar Y\sum X_i= \hat\beta\_2\sum X_i\ ^2- \bar X\sum X_i$$

$$\hat\beta_2=\frac{\sum{X_i Y_i}-\bar Y\sum(X_i)}{\sum(X_i^2) - \bar X\sum(X_i)}$$

# 

### Supestos del Teorema Gauss-Markov

#### 1. Linealidad en los par√°metros

El modelo es lineal en los coeficientes.\
Cada regresor contribuye de manera aditiva y proporcional.

$$
Y_i = \beta_0 + \beta_1 X_{i} + \varepsilon_i
$$

#### 2. Los valores de $X$ son fijos

Establece que los valores de las variables explicativas se consideran fijos en muestras repetidas y son independientes del t√©rmino de error o perturbaci√≥n.

# 

#### 3. Media condicional de los errores

El error tiene esperanza cero.\
El modelo no se inclina sistem√°ticamente hacia arriba o hacia abajo.

$$
E(\varepsilon_i \mid X) = 0
$$

#### 4. Homocedasticidad

La varianza del error es la misma en todos los niveles de $X$.

$$
\operatorname{Var}(\varepsilon_i \mid X) = \sigma^2
$$

# 

#### 5. No autocorrelaci√≥n

Los errores no guardan memoria entre s√≠.\
El error de una observaci√≥n no predice el de otra.

$$
\operatorname{Cov}(\varepsilon_i, \varepsilon_j \mid X) = 0, \quad i \neq j
$$

#### 6. Los errores son no correlacionados con las variables explicativas

*(Exogeneidad / media condicional cero)*

‚ÄúDado lo que s√© de $X$, el promedio del error es cero.‚Äù

$$
E(\varepsilon \mid X) = 0 \quad \Leftrightarrow \quad \operatorname{Cov}(\varepsilon, X_k) = 0
$$

# 

#### 7. El n√∫mero de observaciones debe ser mayor al n√∫mero de par√°metros a estimar

*(Identificaci√≥n y grados de libertad)*

‚ÄúNecesito m√°s datos que inc√≥gnitas para identificar el modelo y medir el error.‚Äù

-   $n =$ n√∫mero de observaciones\
-   $p =$ n√∫mero de par√°metros (incluye intercepto)

**Grados de libertad:**

$$
n - p > 0
$$

# 

#### 8. Los valores de cada vector en $X$ no son iguales

*(No es constante / no todos sus valores son iguales)*

‚ÄúPara identificar el efecto de un regresor, √©ste debe cambiar en los datos.‚Äù

$$
\operatorname{Var}(X_k) > 0
$$

#### 9. El modelo est√° bien especificado

*(Forma funcional y variables relevantes/irrelevantes)*

‚ÄúLa forma del modelo es la adecuada y las variables relevantes est√°n incluidas.‚Äù

$$
E(Y \mid X) = X\beta
$$

# 

#### 10. No hay multicolinealidad perfecta

*(Rango completo de* $X$)

‚ÄúNing√∫n regresor es combinaci√≥n lineal exacta de otros.‚Äù

**Condici√≥n:**

$$
X'X \; \text{es invertible.}
$$

# 

## Errores del modelo de acuerdo a los supuestos

### 1. Multicolinealidad

Correlaci√≥n fuerte entre variables explicativas.

***Consecuencias:*** Varianzas grandes, coeficientes imprecisos, dificultad para interpretar.

***Detecci√≥n:*** R¬≤ alto pero t peque√±os, Correlaciones \> 0.8, FIV \> 10

***Correcci√≥n**:* Eliminar variables correlacionadas , Usar transformaciones, Aumentar muestra

# 

### 2. Heterocedasticidad

Varianza no constante de los errores. ***Consecuencias:*** Errores est√°ndar incorrectos, pruebas t y F poco fiables.

***Detecci√≥n:*** Gr√°ficos de residuos, Pruebas de Park, Glejser y White ***Correcci√≥n:*** M√≠nimos cuadrados ponderados, Errores robustos, Transformaciones logar√≠tmicas

# 

### 3. Autocorrelaci√≥n

Correlaci√≥n entre errores (com√∫n en series de tiempo).

*Consecuencias:* Estimaciones ineficientes, hip√≥tesis inv√°lidas.

*Detecci√≥n:* Gr√°ficos de residuos, Durbin-Watson, Breusch-Godfrey

*Correcci√≥n:* Revisar modelo, Usar MCG/MCGF, Correcci√≥n de Newey-West

# 

### 4.Error de Especificaci√≥n

Es una violaci√≥n al Supuesto 9 del MCRL, que establece que "el modelo est√° correctamente especificado, por lo que no hay sesgo de especificaci√≥n".

*Consecuencias:* Cuando ocurre un error de especificaci√≥n: Se violan los supuestos del MCRL,el estimador de MCO pierde la propiedad de ser MELI(insesgado y de varianza m√≠nima).

# 

### ¬øCu√°ndo no se cumplen los supuestos?

1.  **Error de espec√≠ficaci√≥n.** Este comete una violaci√≥n al supuesto 1 de linealidad en los par√°metros y a su vez el 9 que nos dice que el modelo est√° bien especificado.

2.  **Error de Multicolinealidad.** Va directamente en contra de del supuesto 10 que nos dice que no hay multicolinealidad perfecta. Esto produce coeficientes inestables, errores est√°ndar grandes y mala precisi√≥n en las inferencias.

# 

3.  **Error de Heterocedasticidad.** Este error esta dado por $Var(\epsilon_i)\neq\sigma^2$ Afecta directamente al supuesto 4 que nos dice que la varianza del modelo es homosced√°stica. Lo que afecta la efciencia, los erroes estandar e incluso pueden cometer errores de significancia.

4.  **Error de Autocorrelaci√≥n.** Este error viola el supuesto 6 que nos dice que los errores son no correlacionados con las variables explicativas. Pierde precisi√≥n y confiabilidad en la inferencia estad√≠stica.

# °GRACIAS! {.center}
